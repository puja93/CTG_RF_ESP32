{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb7ad4",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataframe \n",
    "df = pd.read_csv('ctg.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split with ratio\n",
    "#Train:Test = 0.7:0.3\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3339ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features & Labels\n",
    "\n",
    "X_train = train[train.columns[:-1]].values\n",
    "y_train = train[train.columns[-1]].values\n",
    "\n",
    "X_test = test[test.columns[:-1]].values\n",
    "y_test = test[test.columns[-1]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd124f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Construct Model\n",
    "\n",
    "#Declare hyperparameter options\n",
    "model_params = {\n",
    "    'n_estimators': [10, 15, 20, 25],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_features': ['sqrt', 'auto', 'log2', 0.25, 0.5, 0.75, 1.0],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'max_depth': [4,5,6,7],\n",
    "}\n",
    "\n",
    "#Initiate classifier\n",
    "rf_model = RandomForestClassifier(random_state=123)\n",
    "classifier = GridSearchCV(rf_model, model_params, cv=5)\n",
    "\n",
    "#Train the grid search to find the best model\n",
    "model = classifier.fit(X_train, y_train)\n",
    "\n",
    "#Print best hyperparameter set\n",
    "pprint(model.best_estimator_.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258b6d4",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Evaluation Metrics\n",
    "\n",
    "#Predict using test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Confusion matrix\n",
    "print('Confusion Matrix \\n', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#Get evaluation metrics\n",
    "report = classification_report(y_test, y_pred, digits=5, output_dict=True)\n",
    "\n",
    "#Generate report in a dataframe\n",
    "ev_metrics = pd.DataFrame(report).transpose()\n",
    "ev_metrics['support'] = ev_metrics['support'].astype(int)\n",
    "acc = ev_metrics.loc['accuracy'][0]\n",
    "ev_metrics.drop(['accuracy', 'macro avg'], inplace=True)\n",
    "ev_metrics.index = ['Normal', 'Suspect', 'Pathologic', 'Weighted Average']\n",
    "\n",
    "#Style for weighted average\n",
    "ev_metrics.style.set_table_styles({\n",
    "    'Weighted Average': [{'selector': '',\n",
    "         'props': [('border-top', '2px solid black')]}]\n",
    "}, axis=1, overwrite=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecac7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Feature Importance\n",
    "\n",
    "def plot_feature_importance(feature_importance,feature_names,title):\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=df['feature_importance'], y=df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "\n",
    "#Show ranked feature importances\n",
    "plot_feature_importance(model.best_estimator_.feature_importances_,df.columns[:-1],'CTG-RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f8a19",
   "metadata": {},
   "source": [
    "### Edge Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Show all trees in this forest\n",
    "\n",
    "print('Total Trees in this Forest are :', len(model.best_estimator_.estimators_))\n",
    "model.best_estimator_.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435802d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Show the structure of the first tree only\n",
    "\n",
    "#This structure is the one we can use for rewriting the model in C++\n",
    "print('All if-else statements from the tree\\n')\n",
    "print(export_text(model.best_estimator_.estimators_[0], feature_names=list(df.columns[:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Visualize tree if needed\n",
    "\n",
    "# fig = plt.figure(figsize=(100, 100))\n",
    "# plot_tree(model.best_estimator_.estimators_[0], \n",
    "#           feature_names=df.columns[:-1],\n",
    "#           class_names=df.columns[-1], \n",
    "#           filled=True, impurity=True, \n",
    "#           rounded=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ebf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Parse to C++\n",
    "\n",
    "#This micromlgen is a useful parser for the model.\n",
    "#We can also write our own parser, as long as\n",
    "#the model's tree structure can be exported\n",
    "\n",
    "from micromlgen import port\n",
    "\n",
    "#Save best model in a file\n",
    "lib = port(model.best_estimator_, classmap={1:'Normal', 2:'Suspect', 3:'Pathologic'})\n",
    "\n",
    "add_header = '#include \"stdint.h\"\\n'\n",
    "pos = lib.find('namespace')\n",
    "updated_lib = lib[:pos] + add_header + lib[pos:]\n",
    "\n",
    "add_index = '+1' #micromlgen can't start at 1, always 0.\n",
    "pos = lib.find('return classIdx') + len('return classIdx')\n",
    "updated_lib = lib[:pos] + add_index + lib[pos:]\n",
    "\n",
    "with open('src/model.h', 'w') as f:\n",
    "    print(updated_lib, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On linux only, checking model size\n",
    "\n",
    "# !ls -l 'src/model.h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write test file to C++\n",
    "\n",
    "#Downcast Python(float64 to float32) (C++ double to float)\n",
    "test = test.copy()\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "# test = test.round(6)\n",
    "x_str = str(test[test.columns[:-1]].to_numpy().tolist())\n",
    "y_str = str(test[test.columns[-1]].to_numpy().tolist())\n",
    "\n",
    "row = test.shape[0]\n",
    "col = test.shape[1]\n",
    "\n",
    "\n",
    "#Write X_test\n",
    "x_str = x_str.replace(\"[\", \"{\").replace(\"]\", \"}\")\n",
    "with open('src/test_data.cc', 'w') as f:\n",
    "    print(f'float x_test[{row}][{col}]=', file=f)\n",
    "    print(x_str, file=f, end='')\n",
    "    print(';', file=f)\n",
    "    print('\\n', file=f)\n",
    "    \n",
    "    \n",
    "#Append y_test\n",
    "y_str = y_str.replace(\"[\", \"{\").replace(\"]\", \"}\")\n",
    "with open('src/test_data.cc', 'a') as f:\n",
    "    print(f'int y_test[{row}]=', file=f)\n",
    "    print(y_str, file=f, end='')\n",
    "    print(';', file=f)\n",
    "    \n",
    "#Now create the header file (.h)\n",
    "with open('src/test_data.h', 'w') as f:\n",
    "    print(f'extern float x_test[{row}][{col}];', file=f)\n",
    "    print(f'extern int y_test[{row}];', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc9420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
